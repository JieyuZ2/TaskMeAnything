{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Model Comparison Query and Query Results Approximation in Task-Me-Anything\n",
    "\n",
    "In this notebook, we will show how to perform a “Model Comparison Query” in Task-Me-Anything. We’ll compare the performance of `llavav1.5-7b` with the baseline model `instructblip-flant5xl` over 3200+ task plans on “2D sticker how many” task type, by finding the task plan that performance  of `llavav1.5-7b` is significant higher than `instructblip-flant5xl`. After that, we willl using `Fit` and `Active` query results approximation algorithms to approximate the performance of tasks plan within only 500 budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tasks\n",
    "\n",
    "These are the process of task plans generation, illustrations on these part will be in the `generate` part of demo.\n",
    "\n",
    "In this step, we generate 3,249 “how many” task plans in 2D scenarios. Each task plan contains all the configuration and content needed to generate an image-question pair (test instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "enumerating [how many attribute 1] task: 100%|██████████| 3/3 [00:00<00:00, 8955.81it/s]\n",
      "enumerating [how many attribute 2] task: 100%|██████████| 465/465 [00:01<00:00, 241.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task type</th>\n",
       "      <th>grid number</th>\n",
       "      <th>target category</th>\n",
       "      <th>count</th>\n",
       "      <th>attribute type</th>\n",
       "      <th>attribute value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how many</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>color</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>color</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>color</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>color</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>how many</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>color</td>\n",
       "      <td>pink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32440</th>\n",
       "      <td>how many</td>\n",
       "      <td>2</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>4</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32450</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>2</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32460</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>4</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32470</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>6</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32480</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>8</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3249 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      task type  grid number target category  count attribute type  \\\n",
       "0      how many            2            <NA>      1          color   \n",
       "10     how many            3            <NA>      7          color   \n",
       "20     how many            3            <NA>      4          color   \n",
       "30     how many            3            <NA>      1          color   \n",
       "40     how many            2            <NA>      2          color   \n",
       "...         ...          ...             ...    ...            ...   \n",
       "32440  how many            2          Q99895      4          color   \n",
       "32450  how many            3          Q99895      2          color   \n",
       "32460  how many            3          Q99895      4          color   \n",
       "32470  how many            3          Q99895      6          color   \n",
       "32480  how many            3          Q99895      8          color   \n",
       "\n",
       "      attribute value  \n",
       "0                gold  \n",
       "10               gold  \n",
       "20             orange  \n",
       "30              black  \n",
       "40               pink  \n",
       "...               ...  \n",
       "32440           white  \n",
       "32450           white  \n",
       "32460           white  \n",
       "32470           white  \n",
       "32480           white  \n",
       "\n",
       "[3249 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# set the working directory to the root of the project\n",
    "sys.path.append(\"../..\")\n",
    "from tma.imageqa.sticker_2d import *\n",
    "from tma.imageqa.metadata import Objaverse2DMetaData\n",
    "from tma.task_store import TaskStore\n",
    "\n",
    "# the code to download the source data, if you already downloaded the data, you can skip this step\n",
    "# from huggingface_hub import snapshot_download\n",
    "# path = \"../taskverse-source\"\n",
    "# snapshot_download(repo_id=\"weikaih/taskverse-source\", repo_type=\"dataset\", local_dir=path)\n",
    "\n",
    "\n",
    "\n",
    "path = '/your_path/TaskMeAnything-v1-source'\n",
    "metadata = Objaverse2DMetaData('../../annotations', image_folder=f'{path}/object_images')\n",
    "generator = HowManyGridTaskGenerator(metadata)\n",
    "\n",
    "\n",
    "# enumerate all \"how many\" task plans\n",
    "task_store = TaskStore(generator.schema)\n",
    "generator.enumerate_task_plans(task_store)\n",
    "df = task_store.return_df()\n",
    "\n",
    "\n",
    "# sample a subset of the all \"how many\" task plans\n",
    "interval = len(df) // 3000\n",
    "df = df.iloc[::interval, :]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the tasks and create VQATaskEvaluator\n",
    "\n",
    "\n",
    "Task evaluator takes the model and the tasks as input, and evaluate and query the model's performance on the tasks generated by task plans. \n",
    "\n",
    "\n",
    "\n",
    "<!-- Because we want to fit a performance regressor, we need to embed the tasks. We will use the Cohere API to embed the tasks. First you need to set the `api_key` parameter to your Cohere API key. You can also using other embedding API or models to embed the tasks. (e.g Openai embedding API, BERT, etc.)\n",
    "\n",
    "Then you should create a `VQATaskEvaluator` object. `VQATaskEvaluator` is a class designed to evaluate a model's performance on task. It can handle the details in evaluate the model such as create the embedding of the tasks, fit the performance regressor, etc.\n",
    "\n",
    "Notice that `VQATaskEvaluator` can cache the embeddings to avoid redundant requests to the OpenAI API. You can change the path of the cache file by setting the `cache_path` parameter. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tma.task_evaluator import VQATaskEvaluator\n",
    "\n",
    "task_evaluator = VQATaskEvaluator(\n",
    "    task_plan_df=df, # data frames task plans to evaluate\n",
    "    task_generator=generator, # task generator, used to generate test instances for each task plan\n",
    "    embedding_name='st',  # using sentence transformer (st) to embedding questions\n",
    "    embedding_batch_size=10000,  # batch size for embedding\n",
    "    n_instance_per_task=5,  # number of test instances generated per task plan\n",
    "    n_trials_per_instance=3,  # number of trials per test instance\n",
    "    cache_path_root=\".cache\",  # enter you path for cache\n",
    "    seed=42  # random seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on all the task plans\n",
    "\n",
    "In this steps, we will start to get the ground truth of the query. We will not use query approximation algorithms in this step. Instead, we will evaluate the model on all the tasks and get the top 10 worst-performing tasks as the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call tma.models.qa_model.list_vqa_models() to find all the available VQA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instructblip-flant5xl',\n",
       " 'instructblip-flant5xxl',\n",
       " 'instructblip-vicuna7b',\n",
       " 'instructblip-vicuna13b',\n",
       " 'blip2-flant5xxl',\n",
       " 'llavav1.5-7b',\n",
       " 'llavav1.5-13b',\n",
       " 'llavav1.6-34b',\n",
       " 'llava1.6-34b-api',\n",
       " 'qwenvl',\n",
       " 'qwenvl-chat',\n",
       " 'internvl-chat-v1.5',\n",
       " 'gpt4v',\n",
       " 'gpt4o',\n",
       " 'qwen-vl-plus',\n",
       " 'qwen-vl-max',\n",
       " 'gemini-vision-pro']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tma.models.qa_model import list_imageqa_models\n",
    "\n",
    "# list all available models\n",
    "list_imageqa_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `instructblip-flant5xl` as baseline model and `llavav1.5-7b` as model for comparing for showcasing, you can use other models you like or using multi-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IMPORTANT] model cache is enabled, cache path: .cache/\n",
      "Loading instructblip-flant5xl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637e42bb910544358632e2f653465524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading instructblip-flant5xl\n",
      "[IMPORTANT] model cache is enabled, cache path: .cache/\n",
      "Loading llavav1.5-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce8e2dbebcc44cc9a7a3afd9acf0d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading llavav1.5-7b\n"
     ]
    }
   ],
   "source": [
    "from tma.models.qa_model import ImageQAModel\n",
    "from tma.models.qa_model import prompt\n",
    "import torch\n",
    "\n",
    "# single model\n",
    "baseline_model = ImageQAModel(model_name='instructblip-flant5xl', precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "model_to_compare = ImageQAModel(model_name='llavav1.5-7b', precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "\n",
    "\n",
    "# # multiple models\n",
    "# # Notice: If you have multiple GPUs, you can set the torch_device for each model to avoid running out of GPU memory.\n",
    "# model1 = ImageQAModel(model_name='llavav1.5-7b', torch_device=0, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "# model2 = ImageQAModel(model_name='llavav1.5-13b', torch_device=1, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    " \n",
    "# baseline_models = [model1, model2]\n",
    "\n",
    "\n",
    "# model3 = ImageQAModel(model_name='qwenvl', torch_device=3, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "# model4 = ImageQAModel(model_name='qwenvl-chat', torch_device=4, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    " \n",
    "# models_to_compare = [model3, model4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading model, we can start evaluating all the task plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tasks: 100%|██████████| 3249/3249 [05:38<00:00,  9.59it/s] \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# find the task plan that the model_to_compare performs better than the baseline_model above 30%\n",
    "\n",
    "ground_truth_results = task_evaluator.model_compare(\n",
    "    x_indices=np.arange(len(df)),\n",
    "    greater_than=True,\n",
    "    threshold = 0.3,\n",
    "    baselines=[baseline_model],\n",
    "    model = model_to_compare,\n",
    "    fit_function_approximator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern                                                                        Times\n",
      "------------------------------------------------------------------------------------\n",
      "task type: how many                                                            534\n",
      "task type: how many, grid number: 3                                            290\n",
      "task type: how many, attribute type: color                                     285\n",
      "task type: how many, grid number: 2                                            244\n",
      "task type: how many, grid number: 3, attribute type: color                     153\n",
      "task type: how many, count: 1                                                  139\n",
      "task type: how many, grid number: 2, attribute type: color                     132\n",
      "task type: how many, count: 3                                                  112\n",
      "task type: how many, count: 1, attribute type: color                           92\n",
      "task type: how many, grid number: 2, count: 3                                  92\n"
     ]
    }
   ],
   "source": [
    "def display_results(results):\n",
    "    pattern_stats = results[0]\n",
    "    # Determine the headers\n",
    "    headers = [\"Pattern\", \"Times\"]\n",
    "    \n",
    "    # Calculate the maximum length for formatting\n",
    "    max_pattern_length = max(len(str(plan[1])) for plan in pattern_stats)\n",
    "    \n",
    "    # Print the headers\n",
    "    print(f\"{headers[0]:<{max_pattern_length}} {headers[1]}\")\n",
    "    print(\"-\" * (max_pattern_length + len(headers[1]) + 1))\n",
    "    \n",
    "    # Iterate over the task plans and print each plan\n",
    "    for plan in pattern_stats:\n",
    "        task_id, attributes = plan\n",
    "        pattern = ', '.join([f\"{attr[0]}: {attr[1]}\" for attr in attributes])\n",
    "        print(f\"{pattern:<{max_pattern_length}} {task_id}\")\n",
    "        \n",
    "display_results(ground_truth_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply query approximation algorithms\n",
    "Query approximation algorithms means only evaluate model on a subset of tasks and use the result to approximate the performance on the whole task plans.\n",
    "\n",
    "We will use the `Fit` algorithm and `Active` algorithm to approximate the top k worst query, and compare the performance of these two methods with the ground truth. For each algorithm, we will give 500 budgets, which means the approximation algorithm can only evaluate 500 task plans.\n",
    "\n",
    "* In the `Fit` approach, we randomly select 500 task plans and fit the function approximator.\n",
    "* In the `Active` approach, we start with 200 task plans and then gradually add more task plans to the training set based on the function approximator's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the functions to evaluate the approximation results with the ground truth\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compare_metric(gt, pred):\n",
    "\n",
    "    gt_selection = gt[1]\n",
    "    if len(gt_selection) == 0:\n",
    "        a = 1\n",
    "    pred_selection = pred[1]\n",
    "\n",
    "    # Determine the maximum index for array sizing\n",
    "    max_index = max(max(gt_selection, default=0), max(pred_selection, default=0))\n",
    "\n",
    "    # Initialize the labels based on the maximum index\n",
    "    gt_label = np.zeros(max_index + 1)\n",
    "    pred_label = np.zeros(max_index + 1)\n",
    "\n",
    "    for k in gt_selection:\n",
    "        gt_label[k] = 1\n",
    "\n",
    "    for k in pred_selection:\n",
    "        pred_label[k] = 1\n",
    "\n",
    "    f1 = f1_score(gt_label, pred_label) * 100\n",
    "    acc = accuracy_score(gt_label, pred_label) * 100\n",
    "    precision = precision_score(gt_label, pred_label) * 100\n",
    "    recall = recall_score(gt_label, pred_label) * 100\n",
    "\n",
    "    return precision, recall, f1, acc\n",
    "\n",
    "def print_metrics(precision, recall, f1, acc):\n",
    "    print(f\"{'Metric':<15} {'Value':<10}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"{'Precision:':<15} {precision:.2f}%\")\n",
    "    print(f\"{'Recall:':<15} {recall:.2f}%\")\n",
    "    print(f\"{'F1 Score:':<15} {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"Fit\" approximation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tasks: 100%|██████████| 500/500 [00:03<00:00, 144.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric          Value     \n",
      "-------------------------\n",
      "Precision:      64.23%\n",
      "Recall:         14.79%\n",
      "F1 Score:       24.05%\n",
      "Pattern                                                                        Times\n",
      "------------------------------------------------------------------------------------\n",
      "task type: how many                                                            123\n",
      "task type: how many, grid number: 3                                            75\n",
      "task type: how many, attribute type: color                                     73\n",
      "task type: how many, grid number: 2                                            48\n",
      "task type: how many, grid number: 3, attribute type: color                     44\n",
      "task type: how many, count: 1                                                  29\n",
      "task type: how many, grid number: 2, attribute type: color                     29\n",
      "task type: how many, count: 3                                                  24\n",
      "task type: how many, count: 1, attribute type: color                           23\n",
      "task type: how many, grid number: 2, count: 3                                  19\n"
     ]
    }
   ],
   "source": [
    "budget = 500\n",
    "np.random.seed(42)\n",
    "perm = np.random.permutation(len(df))\n",
    "x_indices = perm[:budget]\n",
    "\n",
    "fit_results = task_evaluator.model_compare(\n",
    "    x_indices=x_indices,\n",
    "    greater_than=True,\n",
    "    threshold = 0.2,\n",
    "    baselines=[baseline_model],\n",
    "    model = model_to_compare,\n",
    "    fit_function_approximator=True\n",
    ")\n",
    "precision, recall, f1, acc = compare_metric(ground_truth_results, fit_results)\n",
    "print_metrics(precision, recall, f1, acc)\n",
    "display_results(fit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"Active\" approximation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARMUP] Querying 200 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tasks: 100%|██████████| 200/200 [00:01<00:00, 146.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] Queried 300/300 tasks\n",
      "[(134, [('task type', 'how many')]), (85, [('task type', 'how many'), ('attribute type', 'color')]), (79, [('task type', 'how many'), ('grid number', '3')]), (55, [('task type', 'how many'), ('grid number', '2')]), (47, [('task type', 'how many'), ('grid number', '3'), ('attribute type', 'color')]), (38, [('task type', 'how many'), ('grid number', '2'), ('attribute type', 'color')]), (36, [('task type', 'how many'), ('count', '1')]), (35, [('task type', 'how many'), ('count', '3')]), (26, [('task type', 'how many'), ('count', '1'), ('attribute type', 'color')]), (26, [('task type', 'how many'), ('grid number', '2'), ('count', '3')])]\n",
      "Metric          Value     \n",
      "-------------------------\n",
      "Precision:      70.90%\n",
      "Recall:         17.79%\n",
      "F1 Score:       28.44%\n"
     ]
    }
   ],
   "source": [
    "warmup_budget=200\n",
    "active_results = task_evaluator.active_model_compare(\n",
    "    k=10,\n",
    "    warmup_budget=warmup_budget,\n",
    "    budget=budget-warmup_budget,\n",
    "    greater_than=True,\n",
    "    threshold = 0.2,\n",
    "    baselines=[baseline_model],\n",
    "    model = model_to_compare,\n",
    ")\n",
    "\n",
    "precision, recall, f1, acc = compare_metric(ground_truth_results, active_results)\n",
    "print_metrics(precision, recall, f1, acc)\n",
    "display_results(active_results[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
