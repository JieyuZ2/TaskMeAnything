{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Model Debugging Query and Query Results Approximation in Task-Me-Anything\n",
    "\n",
    "\n",
    "In this notebook, we will show how to perform a “Model Debugging Query” in Task-Me-Anything. We’ll debug the performance of `llavav1.5-7b` on over 3200+ task plans on “2D sticker how many” task type, by finding the task plan whose performance is at least 30% below the average. After that, we willl using `Fit` and `Active` query results approximation algorithms to approximate the performance of tasks plan within only 500 budgets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- In this notebook we will illstrate how to conduct `Top K query` on multiple models in taskverse. Top K query has two types: Top K best query and Top K worst query, which aims to query the top k best or worst test cases.  We will use Top K worst query in this notebook. Top K worst query is to query the top k task categories that given `VQA model` performance at. (e.g GPT4v achieve 0.5 acc in task category 1 and 0.3 in task category 2, then task category 2 is worse than task category 1)\n",
    "\n",
    "\n",
    "In this notebook, we will first evaluates models on all the 2d sticker howmany test cases and got the top 10 worst performed cases as ground truth. Then we will use `Random Selection` and `Active Selection` method to approximate the top 10 worst performed cases. We will compare the performance of these two methods with the ground truth. \n",
    "\n",
    "In the `Random Selection` approach, we randomly select 2000 task categories and train the `performance regressor`. Conversely, in the second method, we iteratively select the top k worst-performing data points and train the `performance regressor` accordingly. We will discuss the details in later sections.\n",
    "\n",
    "It is important to note that the `Active Selection` of top k worst-performaing data is specifically tailored for identifying the top k worst scenarios. Since it is trained using data from the top k worst queries, it may not generalize well to scenarios involving the top k best queries or other requirements. In contrast, the `Random Selection` method offers a more generalized approach. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tasks\n",
    "\n",
    "These are the process of task plans generation, illustrations on these part will be in the `generate` part of demo.\n",
    "\n",
    "In this step, we generate 3,249 “how many” task plans in 2D scenarios. Each task plan contains all the configuration and content needed to generate an image-question pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "enumerating [how many attribute 1] task: 100%|██████████| 3/3 [00:00<00:00, 8848.74it/s]\n",
      "enumerating [how many attribute 2] task: 100%|██████████| 465/465 [00:01<00:00, 261.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task type</th>\n",
       "      <th>grid number</th>\n",
       "      <th>target category</th>\n",
       "      <th>count</th>\n",
       "      <th>attribute type</th>\n",
       "      <th>attribute value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how many</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>color</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>color</td>\n",
       "      <td>gray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>how many</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>color</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32440</th>\n",
       "      <td>how many</td>\n",
       "      <td>2</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>4</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32450</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>2</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32460</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>4</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32470</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>6</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32480</th>\n",
       "      <td>how many</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99895</td>\n",
       "      <td>8</td>\n",
       "      <td>color</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3249 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      task type  grid number target category  count attribute type  \\\n",
       "0      how many            2            <NA>      1          color   \n",
       "10     how many            3            <NA>      7          color   \n",
       "20     how many            3            <NA>      4          color   \n",
       "30     how many            3            <NA>      1          color   \n",
       "40     how many            2            <NA>      2          color   \n",
       "...         ...          ...             ...    ...            ...   \n",
       "32440  how many            2          Q99895      4          color   \n",
       "32450  how many            3          Q99895      2          color   \n",
       "32460  how many            3          Q99895      4          color   \n",
       "32470  how many            3          Q99895      6          color   \n",
       "32480  how many            3          Q99895      8          color   \n",
       "\n",
       "      attribute value  \n",
       "0               white  \n",
       "10              white  \n",
       "20              green  \n",
       "30               gray  \n",
       "40               blue  \n",
       "...               ...  \n",
       "32440           white  \n",
       "32450           white  \n",
       "32460           white  \n",
       "32470           white  \n",
       "32480           white  \n",
       "\n",
       "[3249 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# set the working directory to the root of the project\n",
    "sys.path.append(\"../..\")\n",
    "from tma.imageqa.sticker_2d import *\n",
    "from tma.imageqa.metadata import Objaverse2DMetaData\n",
    "from tma.task_store import TaskStore\n",
    "\n",
    "# the code to download the source data, if you already downloaded the data, you can skip this step\n",
    "# from huggingface_hub import snapshot_download\n",
    "# path = \"../TaskMeAnything-v1-source\"\n",
    "# snapshot_download(repo_id=\"jieyuz2/TaskMeAnything-v1-source\", repo_type=\"dataset\", local_dir=path)\n",
    "\n",
    "\n",
    "\n",
    "path = '/your_path/TaskMeAnything-v1-source'\n",
    "metadata = Objaverse2DMetaData('../../annotations', image_folder=f'{path}/object_images')\n",
    "generator = HowManyGridTaskGenerator(metadata)\n",
    "\n",
    "\n",
    "# enumerate all \"how many\" task plans\n",
    "task_store = TaskStore(generator.schema)\n",
    "generator.enumerate_task_plans(task_store)\n",
    "df = task_store.return_df()\n",
    "\n",
    "\n",
    "# sample a subset of the all \"how many\" task plans\n",
    "interval = len(df) // 3000\n",
    "df = df.iloc[::interval, :]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the tasks and create VQATaskEvaluator\n",
    "\n",
    "\n",
    "Task evaluator takes the model and the tasks as input, and evaluate and query the model's performance on the tasks generated by task plans. \n",
    "\n",
    "\n",
    "\n",
    "<!-- Because we want to fit a performance regressor, we need to embed the tasks. We will use the Cohere API to embed the tasks. First you need to set the `api_key` parameter to your Cohere API key. You can also using other embedding API or models to embed the tasks. (e.g Openai embedding API, BERT, etc.)\n",
    "\n",
    "Then you should create a `VQATaskEvaluator` object. `VQATaskEvaluator` is a class designed to evaluate a model's performance on task. It can handle the details in evaluate the model such as create the embedding of the tasks, fit the performance regressor, etc.\n",
    "\n",
    "Notice that `VQATaskEvaluator` can cache the embeddings to avoid redundant requests to the OpenAI API. You can change the path of the cache file by setting the `cache_path` parameter. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tma.task_evaluator import VQATaskEvaluator\n",
    "\n",
    "task_evaluator = VQATaskEvaluator(\n",
    "    task_plan_df=df, # task plans to evaluate\n",
    "    task_generator=generator, # task generator, used to generate test instances for each task plan\n",
    "    embedding_name='st',  # using sentence transformer to embed questions\n",
    "    embedding_batch_size=10000,  # batch size for embedding\n",
    "    n_instance_per_task=5,  # number of test instances per task plan\n",
    "    n_trials_per_instance=3,  # number of trials per test instance\n",
    "    cache_path_root=\".cache\",  # enter you path for cache\n",
    "    seed=42  # random seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on all the task plans\n",
    "\n",
    "In this steps, we will start to get the ground truth of the query. We will not use query approximation algorithms in this step. Instead, we will evaluate the model on all the tasks and get the top 10 worst-performing tasks as the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call tma.models.qa_model.list_vqa_models() to find all the available VQA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instructblip-flant5xl',\n",
       " 'instructblip-flant5xxl',\n",
       " 'instructblip-vicuna7b',\n",
       " 'instructblip-vicuna13b',\n",
       " 'blip2-flant5xxl',\n",
       " 'llavav1.5-7b',\n",
       " 'llavav1.5-13b',\n",
       " 'llavav1.6-34b',\n",
       " 'llava1.6-34b-api',\n",
       " 'qwenvl',\n",
       " 'qwenvl-chat',\n",
       " 'internvl-chat-v1.5',\n",
       " 'gpt4v',\n",
       " 'gpt4o',\n",
       " 'qwen-vl-plus',\n",
       " 'qwen-vl-max',\n",
       " 'gemini-vision-pro']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tma.models.qa_model import list_imageqa_models\n",
    "\n",
    "# list all available models\n",
    "list_imageqa_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `llavav1.5-7b` for showcasing, you can use other models you like or using multi-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IMPORTANT] model cache is enabled, cache path: .cache/\n",
      "Loading llavav1.5-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c4078a57be4f08ba384d71c985a5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading llavav1.5-7b\n"
     ]
    }
   ],
   "source": [
    "from tma.models.qa_model import ImageQAModel\n",
    "from tma.models.qa_model import prompt\n",
    "import torch\n",
    "\n",
    "# single model\n",
    "model = ImageQAModel(model_name='llavav1.5-7b', precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "\n",
    "# # multiple models\n",
    "# # Notice: If you have multiple GPUs, you can set the torch_device for each model to avoid running out of GPU memory.\n",
    "# model1 = ImageQAModel(model_name='llavav1.5-7b', torch_device=0, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "# model2 = ImageQAModel(model_name='qwenvl-chat', torch_device=1, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    " \n",
    "# model = [model1, model2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading model, we can start evaluating all the task plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tasks: 100%|██████████| 3249/3249 [01:32<00:00, 34.98it/s] \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ground_truth_results = task_evaluator.model_debug(\n",
    "    x_indices=np.arange(len(df)),\n",
    "    greater_than=False,\n",
    "    threshold = 0.3,\n",
    "    model = model,\n",
    "    fit_function_approximator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern                                                                        Times\n",
      "------------------------------------------------------------------------------------\n",
      "task type: how many                                                            1174\n",
      "task type: how many, grid number: 3                                            858\n",
      "task type: how many, attribute type: color                                     562\n",
      "task type: how many, grid number: 3, attribute type: color                     430\n",
      "task type: how many, grid number: 2                                            316\n",
      "task type: how many, grid number: 3, count: 5                                  234\n",
      "task type: how many, count: 4                                                  228\n",
      "task type: how many, attribute type: material                                  203\n",
      "task type: how many, attribute type: shape                                     189\n",
      "task type: how many, count: 2                                                  186\n"
     ]
    }
   ],
   "source": [
    "def display_results(results):\n",
    "    pattern_stats = results[0]\n",
    "    # Determine the headers\n",
    "    headers = [\"Pattern\", \"Times\"]\n",
    "    \n",
    "    # Calculate the maximum length for formatting\n",
    "    max_pattern_length = max(len(str(plan[1])) for plan in pattern_stats)\n",
    "    \n",
    "    # Print the headers\n",
    "    print(f\"{headers[0]:<{max_pattern_length}} {headers[1]}\")\n",
    "    print(\"-\" * (max_pattern_length + len(headers[1]) + 1))\n",
    "    \n",
    "    # Iterate over the task plans and print each plan\n",
    "    for plan in pattern_stats:\n",
    "        task_id, attributes = plan\n",
    "        pattern = ', '.join([f\"{attr[0]}: {attr[1]}\" for attr in attributes])\n",
    "        print(f\"{pattern:<{max_pattern_length}} {task_id}\")\n",
    "        \n",
    "display_results(ground_truth_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply query approximation algorithms\n",
    "Query approximation algorithms means only evaluate model on a subset of tasks and use the result to approximate the performance on the whole task plans.\n",
    "\n",
    "We will use the `Fit` algorithm and `Active` algorithm to approximate the top k worst query, and compare the performance of these two methods with the ground truth. For each algorithm, we will give 500 budgets, which means the approximation algorithm can only evaluate 500 task plans.\n",
    "\n",
    "* In the `Fit` approach, we randomly select 500 task plans and fit the function approximator.\n",
    "* In the `Active` approach, we start with 200 task plans and then gradually add more task plans (10 each steps) to the training set based on the function approximator's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the functions to evaluate the approximation results with the ground truth\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def debug_metric(gt, pred):\n",
    "\n",
    "    gt_selection = gt[1]\n",
    "    if len(gt_selection) == 0:\n",
    "        a = 1\n",
    "    pred_selection = pred[1]\n",
    "\n",
    "    # Determine the maximum index for array sizing\n",
    "    max_index = max(max(gt_selection, default=0), max(pred_selection, default=0))\n",
    "\n",
    "    # Initialize the labels based on the maximum index\n",
    "    gt_label = np.zeros(max_index + 1)\n",
    "    pred_label = np.zeros(max_index + 1)\n",
    "\n",
    "    for k in gt_selection:\n",
    "        gt_label[k] = 1\n",
    "\n",
    "    for k in pred_selection:\n",
    "        pred_label[k] = 1\n",
    "\n",
    "    f1 = f1_score(gt_label, pred_label) * 100\n",
    "    acc = accuracy_score(gt_label, pred_label) * 100\n",
    "    precision = precision_score(gt_label, pred_label) * 100\n",
    "    recall = recall_score(gt_label, pred_label) * 100\n",
    "\n",
    "    return precision, recall, f1, acc\n",
    "\n",
    "def print_metrics(precision, recall, f1, acc):\n",
    "    print(f\"{'Metric':<15} {'Value':<10}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"{'Precision:':<15} {precision:.2f}%\")\n",
    "    print(f\"{'Recall:':<15} {recall:.2f}%\")\n",
    "    print(f\"{'F1 Score:':<15} {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"Fit\" approximation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tasks: 100%|██████████| 500/500 [00:01<00:00, 148.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric          Value     \n",
      "-------------------------\n",
      "Precision:      100.00%\n",
      "Recall:         16.35%\n",
      "F1 Score:       28.11%\n",
      "Pattern                                                                           Times\n",
      "---------------------------------------------------------------------------------------\n",
      "task type: how many                                                               192\n",
      "task type: how many, grid number: 3                                               138\n",
      "task type: how many, attribute type: color                                        83\n",
      "task type: how many, grid number: 3, attribute type: color                        59\n",
      "task type: how many, grid number: 2                                               54\n",
      "task type: how many, attribute type: material                                     42\n",
      "task type: how many, count: 4                                                     40\n",
      "task type: how many, attribute type: shape                                        31\n",
      "task type: how many, grid number: 3, count: 5                                     30\n",
      "task type: how many, grid number: 3, attribute type: material                     29\n"
     ]
    }
   ],
   "source": [
    "# ground_truth\n",
    "\n",
    "budget = 500\n",
    "np.random.seed(42)\n",
    "perm = np.random.permutation(len(df))\n",
    "x_indices = perm[:budget]\n",
    "\n",
    "fit_results = task_evaluator.model_debug(\n",
    "    x_indices=x_indices,\n",
    "    greater_than=False,\n",
    "    threshold = 0.3,\n",
    "    model = model,\n",
    "    fit_function_approximator=True\n",
    ")\n",
    "\n",
    "precision, recall, f1, acc = debug_metric(ground_truth_results, fit_results)\n",
    "print_metrics(precision, recall, f1, acc)\n",
    "display_results(fit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"Active\" approximation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARMUP] Querying 200 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tasks: 100%|██████████| 200/200 [00:01<00:00, 148.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] Queried 300/300 tasks\n",
      "Metric          Value     \n",
      "-------------------------\n",
      "Precision:      100.00%\n",
      "Recall:         16.87%\n",
      "F1 Score:       28.86%\n",
      "Pattern                                                                                Times\n",
      "--------------------------------------------------------------------------------------------\n",
      "task type: how many                                                                    198\n",
      "task type: how many, grid number: 3                                                    155\n",
      "task type: how many, attribute type: color                                             86\n",
      "task type: how many, attribute type: material                                          72\n",
      "task type: how many, grid number: 3, attribute type: color                             66\n",
      "task type: how many, grid number: 3, attribute type: material                          57\n",
      "task type: how many, attribute type: color, attribute value: white                     49\n",
      "task type: how many, grid number: 2                                                    43\n",
      "task type: how many, grid number: 3, count: 5                                          40\n",
      "task type: how many, count: 4                                                          39\n"
     ]
    }
   ],
   "source": [
    "warmup_budget=200\n",
    "active_results = task_evaluator.active_model_debug(\n",
    "    k=10,\n",
    "    warmup_budget=warmup_budget,\n",
    "    budget=budget-warmup_budget,\n",
    "    model=model,\n",
    "    greater_than=False,\n",
    "    threshold = 0.3\n",
    ")\n",
    "\n",
    "precision, recall, f1, acc = debug_metric(ground_truth_results, active_results)\n",
    "print_metrics(precision, recall, f1, acc)\n",
    "display_results(active_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
